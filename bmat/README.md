# Digital - Senior Engineer test


A lot of our work is about connecting digital service providers (DSPs) like 
Spotify or YouTube with societies like SGAE or SACEM, who represent music 
creators. DSPs provide digital sales reports (DSRs), which contain information 
about music metadata and revenue generated. We crunch this data and give 
societies the information they need.

For this test, we provide several DSRs that represent the usages and revenue 
from different countries. The aim is to parse the contents of the DSRs and 
insert them into a database to extract statistics through an API. Each line of 
a DSR represents a sound recording and its associated usage data. In detail, 
it contains the following fields:

    dsp_id: the unique identifier of a sound recording provided by DSP.
    title: sound recording title.
    artists: pipe-separated list of artists.
    isrc: International Sound Recording Code.
    usages: number of plays for this sound recording, territory and period.
    revenue: revenue generated by this sound recording, territory and period.

DSR filenames specify metadata related to the DSR, such as Territory, Period, 
and Currency. You will find the DSRs in the `data/` directory.

The API specification is provided as an OpenAPI specification:

    openapi.md

Our current (and incomplete) database contains the following tables:

* DSR: Models the DSR file and stores some relevant information.
* Currency: Models a currency.
* Territory: Models a territory.

Deliverables:

* A way to import the contents of DSRs to the DB. -> DONE
* Complete the API according to the OpenAPI specification.
* A form in the admin page to delete DSRs and it's contents. -> DONE
* Tests for each api endpoint, using any preferred testing framework. -> DONE
* Dockerfile

Requirements:

* Django 3.1
* Python 3.9

Extra questions:

* DSPs report DSRs containing hundreds of millions of usages. If you were to 
  deploy this solution to production, would you do any change in the database 
  or process, in order to import the usages? Which ones?

* ANSWER FROM CARLOS. I would several things:
  * DB side. First, use just one transaction. It is true that transactions are mainly used to guarantee data integrity (either something happens or not), but I guess time must be saved if one uses one transaction only rather a number of transactions of the order of the records to be inserted. 
  * DB side. The main performance improvement though, will happen if 'bulk_create' is used. In such a way, all records could potentially be inserted in just one query. 
  * DB side. It could also make sense to have a database connection pool. This would keep connections open for use, instead of having to open them every time an operation needs to be performed on the DB, which could be expensive. 
  * App side. Can the DB handle multiple threads that perform insert operations? Different threads could take care of reading different files/records, which I understand are independent from each other, and therefore could be consumed in parallel. 
Note:

In order to manage python dependencies, it will be necessary to use any tool 
(e.g.: pipenv) that interprets the Pipfile placed in the root folder.

For example, using `pipenv`, it's enough to do:

    pipenv sync --dev

